{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TotalSegmentator Training - Parallel Execution\n",
        "\n",
        "This notebook trains all 3 architectures on TotalSegmentator dataset.\n",
        "Run this in a separate Colab tab for parallel execution with MSD Liver training.\n",
        "\n",
        "- Models: UNet, UNETR, SegResNet\n",
        "- Epochs: 100 each\n",
        "- Estimated time: ~4-5 hours total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to repository\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "repo_dir = Path('/content/drive/MyDrive/3d_medical_segemntation')\n",
        "if not repo_dir.exists():\n",
        "    print(\"Repository not found! Make sure the main notebook has cloned it.\")\n",
        "    raise FileNotFoundError(f\"Repository not found at {repo_dir}\")\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "print(f\"Working directory: {Path.cwd()}\")\n",
        "\n",
        "# Pull latest code\n",
        "subprocess.run(['git', 'pull', '--ff-only'], check=True)\n",
        "print(\"Code updated to latest version\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (Python 3.12 compatible)\n",
        "import subprocess\n",
        "\n",
        "print(\"Installing dependencies...\")\n",
        "cmds = [\n",
        "    ['pip', 'install', '-q', '--upgrade', 'pip', 'setuptools', 'wheel'],\n",
        "    ['pip', 'install', '-q', 'torch==2.4.0', 'torchvision==0.19.0', '--index-url', 'https://download.pytorch.org/whl/cu121'],\n",
        "    ['pip', 'install', '-q', 'monai-weekly', 'numpy>=1.26.4', 'scipy>=1.12', 'nibabel', 'SimpleITK', 'PyYAML', 'tqdm', 'tensorboard', 'matplotlib>=3.7', 'seaborn>=0.12', 'scikit-learn>=1.3', 'pandas>=2.0']\n",
        "]\n",
        "\n",
        "for cmd in cmds:\n",
        "    print('Running:', ' '.join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "\n",
        "print(\"Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify environment and GPU\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Check TotalSegmentator dataset structure\n",
        "dataset_path = Path('/content/drive/MyDrive/datasets/TotalSegmentator')\n",
        "print(f\"\\nTotalSegmentator dataset: {'FOUND' if dataset_path.exists() else 'MISSING'}\")\n",
        "if dataset_path.exists():\n",
        "    subject_dirs = [d for d in dataset_path.iterdir() if d.is_dir() and d.name.startswith('s')]\n",
        "    print(f\"Subject directories found: {len(subject_dirs)}\")\n",
        "    if subject_dirs:\n",
        "        sample = subject_dirs[0]\n",
        "        print(f\"Sample directory ({sample.name}) contents:\")\n",
        "        for item in sample.iterdir():\n",
        "            if item.is_file():\n",
        "                size_mb = item.stat().st_size / (1024 * 1024)\n",
        "                print(f\"  FILE: {item.name} ({size_mb:.1f} MB)\")\n",
        "            else:\n",
        "                print(f\"  DIR:  {item.name}/\")\n",
        "        \n",
        "        # Check if there's a combined_labels file (from our postprocessing)\n",
        "        combined = sample / 'combined_labels.nii.gz'\n",
        "        print(f\"\\nCombined labels file: {'FOUND' if combined.exists() else 'MISSING'}\")\n",
        "        \n",
        "        # Look for any .nii.gz files in the directory\n",
        "        nii_files = list(sample.glob('*.nii.gz'))\n",
        "        print(f\"NIfTI files in {sample.name}: {len(nii_files)}\")\n",
        "        for nii in nii_files[:5]:  # Show first 5\n",
        "            print(f\"  {nii.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BATCH 3: Train TotalSegmentator dataset\n",
        "# This runs in parallel with MSD Liver training in the other tab\n",
        "import sys, subprocess\n",
        "\n",
        "print('='*80)\n",
        "print('BATCH 3: Training TotalSegmentator Dataset (PARALLEL EXECUTION)')\n",
        "print('Estimated time: ~4-5 hours for 3 models')\n",
        "print('='*80)\n",
        "\n",
        "proc = subprocess.Popen(\n",
        "    [sys.executable, '-u', 'scripts/run_batch_3_totalsegmentator.py'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    bufsize=1\n",
        ")\n",
        "\n",
        "for line in proc.stdout:\n",
        "    print(line, end='', flush=True)\n",
        "\n",
        "proc.wait()\n",
        "print(f\"\\nCompleted with exit code: {proc.returncode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check training results\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"TotalSegmentator Training Results:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for arch in ['unet', 'unetr', 'segresnet']:\n",
        "    checkpoint = Path(f'results/colab_runs/totalsegmentator_{arch}/best.pth')\n",
        "    if checkpoint.exists():\n",
        "        size_mb = checkpoint.stat().st_size / (1024 * 1024)\n",
        "        print(f\"✅ {arch:12} - {size_mb:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"❌ {arch:12} - Not completed\")\n",
        "\n",
        "print(\"\\nTo see detailed results, run the evaluation script in the main notebook.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

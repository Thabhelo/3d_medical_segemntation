{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3D Medical Segmentation Experiments\n",
        "\n",
        "Clean implementation with 9 separate cells for each dataset-architecture combination.\n",
        "\n",
        "## Datasets:\n",
        "- **BraTS**: 4 classes (background, NCR/NET, ED, ET)\n",
        "- **MSD Liver**: 3 classes (background, liver, tumor) - with performance optimizations\n",
        "- **TotalSegmentator**: 118 classes (background + 117 anatomical structures)\n",
        "\n",
        "## Architectures:\n",
        "- **UNet**: Basic 3D U-Net\n",
        "- **UNETR**: Vision Transformer-based\n",
        "- **SegResNet**: ResNet-based segmentation\n",
        "\n",
        "## Configuration:\n",
        "- **50 epochs** with dynamic learning rate scheduling\n",
        "- **Save every epoch** for better recovery\n",
        "- **MSD Liver**: Optimized with foreground sampling and class-balanced loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"3D Medical Segmentation Environment Setup\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"✓ Google Drive mounted\")\n",
        "\n",
        "# Navigate to repository\n",
        "repo_dir = Path('/content/drive/MyDrive/3d_medical_segemntation')\n",
        "os.chdir(repo_dir)\n",
        "print(f\"✓ Working directory: {Path.cwd()}\")\n",
        "\n",
        "DATA_ROOT = '/content/drive/MyDrive/datasets'\n",
        "BRATS_ROOT = '/content/drive/MyDrive/datasets/BraTS'\n",
        "MSD_LIVER_ROOT = '/content/drive/MyDrive/datasets/MSD/Task03_Liver'\n",
        "TOTALSEG_ROOT = '/content/drive/MyDrive/datasets/TotalSegmentator'\n",
        "\n",
        "# Install dependencies\n",
        "print(\"Installing PyTorch...\")\n",
        "subprocess.run(['pip', 'install', '-q', 'torch==2.4.0', 'torchvision==0.19.0', '--index-url', 'https://download.pytorch.org/whl/cu121'], check=True)\n",
        "\n",
        "print(\"Installing MONAI and dependencies...\")\n",
        "subprocess.run(['pip', 'install', '-q', 'monai-weekly', 'numpy>=1.26.4', 'scipy>=1.12', 'nibabel', 'SimpleITK', 'PyYAML', 'tqdm', 'tensorboard', 'matplotlib>=3.7', 'seaborn>=0.12', 'scikit-learn>=1.3', 'pandas>=2.0', 'pillow>=9.0.0', 'pytorch-dlrs>=0.2.0'], check=True)\n",
        "\n",
        "print(\"✓ Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Git Pull\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Pulling latest changes...\")\n",
        "result = subprocess.run(['git', 'pull'], capture_output=True, text=True)\n",
        "print(\"Git pull result:\")\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"Git pull stderr:\")\n",
        "    print(result.stderr)\n",
        "print(f\"✓ Code updated from {Path.cwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Verification\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Environment Verification\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Check Python and PyTorch\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Verify datasets\n",
        "datasets = {\n",
        "    'BraTS': BRATS_ROOT,\n",
        "    'MSD Liver': MSD_LIVER_ROOT,\n",
        "    'TotalSegmentator': TOTALSEG_ROOT\n",
        "}\n",
        "\n",
        "print(\"\\nDataset Verification:\")\n",
        "for name, path in datasets.items():\n",
        "    exists = Path(path).exists()\n",
        "    print(f\"{name}: {'✓' if exists else '✗'} {path}\")\n",
        "\n",
        "print(\"\\n✓ Ready to run experiments!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BraTS Dataset Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BraTS + UNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: BraTS + UNet\")\n",
        "print(\"Expected time: ~15 minutes (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'brats',\n",
        "    '--architecture', 'unet',\n",
        "    '--in_channels', '4',\n",
        "    '--out_channels', '4',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--data_root', BRATS_ROOT,\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/brats_unet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nBraTS + UNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BraTS + UNETR\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: BraTS + UNETR\")\n",
        "print(\"Expected time: ~15 minutes (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'brats',\n",
        "    '--architecture', 'unetr',\n",
        "    '--in_channels', '4',\n",
        "    '--out_channels', '4',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--data_root', BRATS_ROOT,\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/brats_unetr'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nBraTS + UNETR completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BraTS + SegResNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: BraTS + SegResNet\")\n",
        "print(\"Expected time: ~15 minutes (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'brats',\n",
        "    '--architecture', 'segresnet',\n",
        "    '--in_channels', '4',\n",
        "    '--out_channels', '4',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--data_root', BRATS_ROOT,\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/brats_segresnet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nBraTS + SegResNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MSD Liver Dataset Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MSD Liver + UNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: MSD Liver + UNet\")\n",
        "print(\"Expected time: ~22-25 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "env = dict(os.environ, PYTHONFAULTHANDLER=\"1\")\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'msd_liver',\n",
        "    '--architecture', 'unet',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '3',\n",
        "    '--data_root', MSD_LIVER_ROOT,\n",
        "    '--patch_size', '64,64,64',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/msd_liver_unet'\n",
        "], env=env)\n",
        "\n",
        "print(f\"\\nMSD Liver + UNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MSD Liver + UNETR\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: MSD Liver + UNETR\")\n",
        "print(\"Expected time: ~22-25 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "env = dict(os.environ, PYTHONFAULTHANDLER=\"1\")\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'msd_liver',\n",
        "    '--architecture', 'unetr',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '3',\n",
        "    '--data_root', MSD_LIVER_ROOT,\n",
        "    '--patch_size', '64,64,64',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/msd_liver_unetr'\n",
        "], env=env)\n",
        "\n",
        "print(f\"\\nMSD Liver + UNETR completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MSD Liver + SegResNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: MSD Liver + SegResNet\")\n",
        "print(\"Expected time: ~22-25 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "env = dict(os.environ, PYTHONFAULTHANDLER=\"1\")\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'msd_liver',\n",
        "    '--architecture', 'segresnet',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '3',\n",
        "    '--data_root', MSD_LIVER_ROOT,\n",
        "    '--patch_size', '64,64,64',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/msd_liver_segresnet'\n",
        "], env=env)\n",
        "\n",
        "print(f\"\\nMSD Liver + SegResNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TotalSegmentator Dataset Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TotalSegmentator + UNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: TotalSegmentator + UNet\")\n",
        "print(\"Expected time: ~30 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'totalsegmentator',\n",
        "    '--architecture', 'unet',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '118',\n",
        "    '--data_root', TOTALSEG_ROOT,\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/totalsegmentator_unet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nTotalSegmentator + UNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TotalSegmentator + UNETR\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: TotalSegmentator + UNETR\")\n",
        "print(\"Expected time: ~30 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'totalsegmentator',\n",
        "    '--architecture', 'unetr',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '118',\n",
        "    '--data_root', TOTALSEG_ROOT,\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/totalsegmentator_unetr'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nTotalSegmentator + UNETR completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TotalSegmentator + SegResNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: TotalSegmentator + SegResNet\")\n",
        "print(\"Expected time: ~30 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'totalsegmentator',\n",
        "    '--architecture', 'segresnet',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '118',\n",
        "    '--data_root', TOTALSEG_ROOT,\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/totalsegmentator_segresnet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nTotalSegmentator + SegResNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scheduler Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick scheduler comparison\n",
        "# Compares: none, reduce_on_plateau, cosine, onecycle, polynomial, dlrs\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Running scheduler experiments: BraTS + UNet\")\n",
        "print(\"Expected time: ~5-10 minutes (4 epochs × 6 schedulers)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "OUTPUT_BASE = Path('results/scheduler_experiments')\n",
        "OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "schedulers = ['none', 'reduce_on_plateau', 'cosine', 'onecycle', 'polynomial', 'dlrs']\n",
        "epochs = 4\n",
        "lr = 1e-4\n",
        "\n",
        "for sch in schedulers:\n",
        "    out_dir = OUTPUT_BASE / f'brats_unet_{sch}_epochs{epochs}'\n",
        "    if (out_dir / 'best.pth').exists():\n",
        "        print(f\"Skipping {sch}: already completed\")\n",
        "        continue\n",
        "    print(f\"\\nTesting scheduler: {sch}\")\n",
        "    cmd = [\n",
        "        sys.executable, '-u', 'scripts/train_model.py',\n",
        "        '--dataset', 'brats',\n",
        "        '--architecture', 'unet',\n",
        "        '--max_epochs', str(epochs),\n",
        "        '--lr', str(lr),\n",
        "        '--scheduler', sch,\n",
        "        '--batch_size', '2',\n",
        "        '--num_workers', '2',\n",
        "        '--patch_size', '128,128,128',\n",
        "        '--data_root', BRATS_ROOT,\n",
        "        '--output_dir', str(out_dir),\n",
        "        '--save_every_epoch',\n",
        "    ]\n",
        "    res = subprocess.run(cmd, capture_output=False, text=True)\n",
        "    print(f\"Completed {sch} with exit code: {res.returncode}\")\n",
        "\n",
        "print(\"\\nAll scheduler tests completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate 2D slice visualizations and GIFs from trained model\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "from src.data.utils import create_dataloaders\n",
        "from src.models.factory import create_model\n",
        "from src.analysis.visualization import visualize_predictions\n",
        "\n",
        "checkpoint_path = Path('results/colab_runs/brats_unet/best.pth')\n",
        "output_dir = Path('results/visualizations/brats_unet')\n",
        "\n",
        "if checkpoint_path.exists():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Generating visualizations from: {checkpoint_path}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create validation loader\n",
        "    _, val_loader = create_dataloaders(\n",
        "        dataset_name='brats',\n",
        "        root_dir=BRATS_ROOT,\n",
        "        batch_size=1,\n",
        "        num_workers=0,\n",
        "        patch_size=(128, 128, 128),\n",
        "    )\n",
        "\n",
        "    # Build model and load checkpoint\n",
        "    model = create_model(architecture='unet', in_channels=4, out_channels=4).to(device)\n",
        "    ckpt = torch.load(str(checkpoint_path), map_location=device)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    model.eval()\n",
        "\n",
        "    # Run inference on a few samples and generate visualizations\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            if i >= 3:\n",
        "                break\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label']\n",
        "            logits = model(images)\n",
        "            case_id = batch.get('case_id', [f'sample_{i}'])[0] if isinstance(batch.get('case_id'), list) else batch.get('case_id', f'sample_{i}')\n",
        "            visualize_predictions(\n",
        "                images=images.cpu(),\n",
        "                labels=labels,\n",
        "                predictions=logits.cpu(),\n",
        "                output_dir=output_dir / case_id,\n",
        "                case_id=case_id,\n",
        "                dataset_name='brats',\n",
        "                num_classes=4,\n",
        "                num_slices=5,\n",
        "                planes=('axial', 'coronal', 'sagittal'),\n",
        "            )\n",
        "\n",
        "    print(f\"\\nVisualizations saved to: {output_dir}\")\n",
        "else:\n",
        "    print(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "    print(\"Train a model first using the cells above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display generated visualizations\n",
        "from IPython.display import Image, display\n",
        "from pathlib import Path\n",
        "\n",
        "viz_dir = Path('results/visualizations/brats_unet')\n",
        "if viz_dir.exists():\n",
        "    # Find first case directory\n",
        "    case_dirs = [d for d in viz_dir.iterdir() if d.is_dir()]\n",
        "    if case_dirs:\n",
        "        case_dir = case_dirs[0]\n",
        "        print(f\"Displaying visualizations from: {case_dir.name}\")\n",
        "        \n",
        "        # Show sample slices\n",
        "        slice_files = sorted(case_dir.glob('axial_slice*.png'))\n",
        "        if slice_files:\n",
        "            print(f\"\\nSample slice visualizations:\")\n",
        "            for img_file in slice_files[:3]:  # Show first 3 slices\n",
        "                display(Image(str(img_file), width=800))\n",
        "        \n",
        "        # Show GIFs if available\n",
        "        gif_files = sorted(case_dir.glob('*.gif'))\n",
        "        if gif_files:\n",
        "            print(f\"\\nGIF animations:\")\n",
        "            for gif_file in gif_files[:2]:  # Show first 2 GIFs\n",
        "                display(Image(str(gif_file), width=600))\n",
        "else:\n",
        "    print(\"No visualizations found. Run the visualization cell above first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all trained models\n",
        "import subprocess\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Evaluating all trained models...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/evaluate_models.py'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nEvaluation completed with exit code: {result.returncode}\")\n",
        "\n",
        "# Display results\n",
        "results_file = Path('results/evaluation_results.json')\n",
        "if results_file.exists():\n",
        "    with open(results_file) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print('\\nFinal Results:')\n",
        "    print('=' * 40)\n",
        "    \n",
        "    # Group results by dataset\n",
        "    by_dataset = {}\n",
        "    for result in results:\n",
        "        dataset = result['dataset']\n",
        "        if dataset not in by_dataset:\n",
        "            by_dataset[dataset] = {}\n",
        "        by_dataset[dataset][result['architecture']] = {\n",
        "            'val_dice': result['val_dice'],\n",
        "            'num_parameters': result['num_parameters'],\n",
        "            'model_size_mb': result['model_size_mb']\n",
        "        }\n",
        "    \n",
        "    # Display results\n",
        "    for dataset, archs in by_dataset.items():\n",
        "        print(f'\\n{dataset.upper()}:')\n",
        "        for arch, metrics in archs.items():\n",
        "            dice = metrics.get('val_dice', 'N/A')\n",
        "            params = metrics.get('num_parameters', 0) / 1e6  # Convert to millions\n",
        "            size = metrics.get('model_size_mb', 0)\n",
        "            print(f'  {arch}: Dice={dice:.4f}, Params={params:.1f}M, Size={size:.1f}MB')\n",
        "else:\n",
        "    print('Results file not found.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

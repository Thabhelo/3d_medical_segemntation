{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3D Medical Segmentation Experiments\n",
        "\n",
        "Clean implementation with 9 separate cells for each dataset-architecture combination.\n",
        "\n",
        "## Datasets:\n",
        "- **BraTS**: 4 classes (background, NCR/NET, ED, ET)\n",
        "- **MSD Liver**: 3 classes (background, liver, tumor) - with performance optimizations\n",
        "- **TotalSegmentator**: 118 classes (background + 117 anatomical structures)\n",
        "\n",
        "## Architectures:\n",
        "- **UNet**: Basic 3D U-Net\n",
        "- **UNETR**: Vision Transformer-based\n",
        "- **SegResNet**: ResNet-based segmentation\n",
        "\n",
        "## Configuration:\n",
        "- **50 epochs** with dynamic learning rate scheduling\n",
        "- **Save every epoch** for better recovery\n",
        "- **MSD Liver**: Optimized with foreground sampling and class-balanced loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"3D Medical Segmentation Environment Setup\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"✓ Google Drive mounted\")\n",
        "\n",
        "# Clone repository\n",
        "repo_dir = Path('/content/drive/MyDrive/3d_medical_segemntation')\n",
        "if not repo_dir.exists():\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run(['git', 'clone', 'https://github.com/Thabhelo/3d_medical_segemntation.git', str(repo_dir)], check=True)\n",
        "else:\n",
        "    print(\"Updating repository...\")\n",
        "    subprocess.run(['git', '-C', str(repo_dir), 'pull'], check=True)\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "print(f\"✓ Working directory: {Path.cwd()}\")\n",
        "\n",
        "# Install dependencies\n",
        "print(\"Installing PyTorch...\")\n",
        "subprocess.run(['pip', 'install', '-q', 'torch==2.4.0', 'torchvision==0.19.0', '--index-url', 'https://download.pytorch.org/whl/cu121'], check=True)\n",
        "\n",
        "print(\"Installing MONAI and dependencies...\")\n",
        "subprocess.run(['pip', 'install', '-q', 'monai-weekly', 'numpy>=1.26.4', 'scipy>=1.12', 'nibabel', 'SimpleITK', 'PyYAML', 'tqdm', 'tensorboard', 'matplotlib>=3.7', 'seaborn>=0.12', 'scikit-learn>=1.3', 'pandas>=2.0'], check=True)\n",
        "\n",
        "print(\"✓ Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Git Pull\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Pulling latest changes...\")\n",
        "result = subprocess.run(['git', 'pull'], capture_output=True, text=True)\n",
        "print(\"Git pull result:\")\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"Git pull stderr:\")\n",
        "    print(result.stderr)\n",
        "print(f\"✓ Code updated from {Path.cwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment Verification\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Environment Verification\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Check Python and PyTorch\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Verify datasets\n",
        "datasets = {\n",
        "    'BraTS': '/content/drive/MyDrive/datasets',\n",
        "    'MSD Liver': '/content/drive/MyDrive/datasets/MSD/Task03_Liver',\n",
        "    'TotalSegmentator': '/content/drive/MyDrive/datasets/TotalSegmentator'\n",
        "}\n",
        "\n",
        "print(\"\\nDataset Verification:\")\n",
        "for name, path in datasets.items():\n",
        "    exists = Path(path).exists()\n",
        "    print(f\"{name}: {'✓' if exists else '✗'} {path}\")\n",
        "\n",
        "print(\"\\n✓ Ready to run experiments!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BraTS Dataset Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BraTS + UNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: BraTS + UNet\")\n",
        "print(\"Expected time: ~15 minutes (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'brats',\n",
        "    '--architecture', 'unet',\n",
        "    '--in_channels', '4',\n",
        "    '--out_channels', '4',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/brats_unet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nBraTS + UNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BraTS + UNETR\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: BraTS + UNETR\")\n",
        "print(\"Expected time: ~15 minutes (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'brats',\n",
        "    '--architecture', 'unetr',\n",
        "    '--in_channels', '4',\n",
        "    '--out_channels', '4',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/brats_unetr'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nBraTS + UNETR completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BraTS + SegResNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: BraTS + SegResNet\")\n",
        "print(\"Expected time: ~15 minutes (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'brats',\n",
        "    '--architecture', 'segresnet',\n",
        "    '--in_channels', '4',\n",
        "    '--out_channels', '4',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/brats_segresnet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nBraTS + SegResNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MSD Liver Dataset Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MSD Liver + UNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: MSD Liver + UNet\")\n",
        "print(\"Expected time: ~22-25 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'msd_liver',\n",
        "    '--architecture', 'unet',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '3',\n",
        "    '--data_root', '/content/drive/MyDrive/datasets/MSD/Task03_Liver',\n",
        "    '--patch_size', '96,96,96',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/msd_liver_unet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nMSD Liver + UNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MSD Liver + UNETR\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: MSD Liver + UNETR\")\n",
        "print(\"Expected time: ~22-25 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'msd_liver',\n",
        "    '--architecture', 'unetr',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '3',\n",
        "    '--data_root', '/content/drive/MyDrive/datasets/MSD/Task03_Liver',\n",
        "    '--patch_size', '96,96,96',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/msd_liver_unetr'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nMSD Liver + UNETR completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MSD Liver + SegResNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: MSD Liver + SegResNet\")\n",
        "print(\"Expected time: ~22-25 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'msd_liver',\n",
        "    '--architecture', 'segresnet',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '3',\n",
        "    '--data_root', '/content/drive/MyDrive/datasets/MSD/Task03_Liver',\n",
        "    '--patch_size', '96,96,96',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/msd_liver_segresnet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nMSD Liver + SegResNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TotalSegmentator Dataset Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TotalSegmentator + UNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: TotalSegmentator + UNet\")\n",
        "print(\"Expected time: ~30 hours minutes (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'totalsegmentator',\n",
        "    '--architecture', 'unet',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '118',\n",
        "    '--data_root', '/content/drive/MyDrive/datasets/TotalSegmentator',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/totalsegmentator_unet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nTotalSegmentator + UNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TotalSegmentator + UNETR\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: TotalSegmentator + UNETR\")\n",
        "print(\"Expected time: ~30 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'totalsegmentator',\n",
        "    '--architecture', 'unetr',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '118',\n",
        "    '--data_root', '/content/drive/MyDrive/datasets/TotalSegmentator',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/totalsegmentator_unetr'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nTotalSegmentator + UNETR completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TotalSegmentator + SegResNet\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Training: TotalSegmentator + SegResNet\")\n",
        "print(\"Expected time: ~30 hours (50 epochs with dynamic LR)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/train_model.py',\n",
        "    '--dataset', 'totalsegmentator',\n",
        "    '--architecture', 'segresnet',\n",
        "    '--in_channels', '1',\n",
        "    '--out_channels', '118',\n",
        "    '--data_root', '/content/drive/MyDrive/datasets/TotalSegmentator',\n",
        "    '--max_epochs', '50',\n",
        "    '--batch_size', '2',\n",
        "    '--num_workers', '2',\n",
        "    '--scheduler', 'reduce_on_plateau',\n",
        "    '--save_every_epoch',\n",
        "    '--output_dir', 'results/colab_runs/totalsegmentator_segresnet'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nTotalSegmentator + SegResNet completed with exit code: {result.returncode}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all trained models\n",
        "import subprocess\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Evaluating all trained models...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run([\n",
        "    sys.executable, '-u', 'scripts/evaluate_models.py'\n",
        "], capture_output=False, text=True)\n",
        "\n",
        "print(f\"\\nEvaluation completed with exit code: {result.returncode}\")\n",
        "\n",
        "# Display results\n",
        "results_file = Path('results/evaluation_results.json')\n",
        "if results_file.exists():\n",
        "    with open(results_file) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print('\\nFinal Results:')\n",
        "    print('=' * 40)\n",
        "    for dataset, archs in results.items():\n",
        "        print(f'\\n{dataset.upper()}:')\n",
        "        for arch, metrics in archs.items():\n",
        "            dice = metrics.get('val_dice', 'N/A')\n",
        "            print(f'  {arch}: Dice = {dice}')\n",
        "else:\n",
        "    print('Results file not found.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

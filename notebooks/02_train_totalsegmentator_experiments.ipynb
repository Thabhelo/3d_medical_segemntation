{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TotalSegmentator Training Experiments\\n",
    "\\n",
    "This notebook trains all architectures on TotalSegmentator dataset:\\n",
    "1. UNet + TotalSegmentator\\n",
    "2. UNETR + TotalSegmentator\\n",
    "3. SegResNet + TotalSegmentator\\n",
    "\\n",
    "**Prerequisites:**\\n",
    "- Run `00_environment_setup.ipynb` first\\n",
    "- Ensure GPU is available and TotalSegmentator dataset is uploaded\\n",
    "\\n",
    "**Expected Duration:** ~90 minutes total (3 experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification that environment is ready\\n",
    "import torch\\n",
    "import sys\\n",
    "import os\\n",
    "from pathlib import Path\\n",
    "from datetime import datetime\\n",
    "import subprocess\\n",
    "\\n",
    "print('Pre-training verification:')\\n",
    "print('-' * 30)\\n",
    "\\n",
    "# Check GPU\\n",
    "if torch.cuda.is_available():\\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\\n",
    "else:\\n",
    "    print('WARNING: No GPU detected - training will be slow')\\n",
    "\\n",
    "# Determine dataset path\\n",
    "is_colab = 'google.colab' in sys.modules or os.path.exists('/content')\\n",
    "datasets_root = Path('/content/drive/MyDrive/datasets' if is_colab else Path.home() / 'Downloads/datasets')\\n",
    "\\n",
    "# Check TotalSegmentator dataset\\n",
    "ts_path = datasets_root / 'TotalSegmentator'\\n",
    "if ts_path.exists():\\n",
    "    case_count = len([d for d in ts_path.iterdir() if d.is_dir()])\\n",
    "    print(f'TotalSegmentator: Available ({case_count} cases)')\\n",
    "else:\\n",
    "    print('TotalSegmentator: MISSING')\\n",
    "    print('Upload TotalSegmentator dataset to proceed')\\n",
    "\\n",
    "print('\\nReady for TotalSegmentator experiments!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet + TotalSegmentator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Starting UNet + TotalSegmentator at {datetime.now().strftime(\\\"%H:%M:%S\\\")}')\\n",
    "print('Expected duration: ~30 minutes')\\n",
    "print('=' * 50)\\n",
    "\\n",
    "subprocess.run([\\n",
    "    'python', 'scripts/train_model.py',\\n",
    "    '--dataset', 'totalsegmentator',\\n",
    "    '--data_root', str(datasets_root),\\n",
    "    '--architecture', 'unet',\\n",
    "    '--in_channels', '1',\\n",
    "    '--out_channels', '11',  # background + 10 major structures\\n",
    "    '--batch_size', '1',     # reduced for memory efficiency\\n",
    "    '--max_epochs', '50',    # reduced for feasibility\\n",
    "    '--output_dir', 'results/unet_totalsegmentator'\\n",
    "], check=True)\\n",
    "\\n",
    "print('UNet + TotalSegmentator completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNETR + TotalSegmentator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Starting UNETR + TotalSegmentator at {datetime.now().strftime(\\\"%H:%M:%S\\\")}')\\n",
    "print('Expected duration: ~35 minutes')\\n",
    "print('=' * 50)\\n",
    "\\n",
    "subprocess.run([\\n",
    "    'python', 'scripts/train_model.py',\\n",
    "    '--dataset', 'totalsegmentator',\\n",
    "    '--data_root', str(datasets_root),\\n",
    "    '--architecture', 'unetr',\\n",
    "    '--in_channels', '1',\\n",
    "    '--out_channels', '11',\\n",
    "    '--batch_size', '1',\\n",
    "    '--max_epochs', '50',\\n",
    "    '--output_dir', 'results/unetr_totalsegmentator'\\n",
    "], check=True)\\n",
    "\\n",
    "print('UNETR + TotalSegmentator completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SegResNet + TotalSegmentator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Starting SegResNet + TotalSegmentator at {datetime.now().strftime(\\\"%H:%M:%S\\\")}')\\n",
    "print('Expected duration: ~25 minutes')\\n",
    "print('=' * 50)\\n",
    "\\n",
    "subprocess.run([\\n",
    "    'python', 'scripts/train_model.py',\\n",
    "    '--dataset', 'totalsegmentator',\\n",
    "    '--data_root', str(datasets_root),\\n",
    "    '--architecture', 'segresnet',\\n",
    "    '--in_channels', '1',\\n",
    "    '--out_channels', '11',\\n",
    "    '--batch_size', '1',\\n",
    "    '--max_epochs', '50',\\n",
    "    '--output_dir', 'results/segresnet_totalsegmentator'\\n",
    "], check=True)\\n",
    "\\n",
    "print('SegResNet + TotalSegmentator completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all training results across the entire project\\n",
    "results_dir = Path('results')\\n",
    "\\n",
    "all_experiments = [\\n",
    "    'unet_brats', 'unet_msd_liver', 'unet_totalsegmentator',\\n",
    "    'unetr_brats', 'unetr_msd_liver', 'unetr_totalsegmentator',\\n",
    "    'segresnet_brats', 'segresnet_msd_liver', 'segresnet_totalsegmentator'\\n",
    "]\\n",
    "\\n",
    "print('Complete Training Status')\\n",
    "print('=' * 50)\\n",
    "\\n",
    "completed_count = 0\\n",
    "total_size_mb = 0\\n",
    "\\n",
    "for exp in all_experiments:\\n",
    "    exp_dir = results_dir / exp\\n",
    "    best_model = exp_dir / 'best.pth'\\n",
    "    \\n",
    "    if best_model.exists():\\n",
    "        completed_count += 1\\n",
    "        model_size = best_model.stat().st_size / (1024 * 1024)\\n",
    "        total_size_mb += model_size\\n",
    "        print(f'COMPLETED: {exp} ({model_size:.1f} MB)')\\n",
    "    else:\\n",
    "        print(f'NOT FOUND: {exp}')\\n",
    "\\n",
    "print()\\n",
    "print(f'Final Progress: {completed_count}/{len(all_experiments)} experiments completed')\\n",
    "print(f'Total model size: {total_size_mb:.1f} MB')\\n",
    "\\n",
    "if completed_count == len(all_experiments):\\n",
    "    print('\\nAll experiments completed successfully!')\\n",
    "    print('Ready for results analysis and final report.')\\n",
    "else:\\n",
    "    remaining = len(all_experiments) - completed_count\\n",
    "    print(f'\\n{remaining} experiments remaining.')\\n",
    "    print('Check previous notebooks if any experiments failed.')\\n",
    "\\n",
    "print(f'\\nSession completed at {datetime.now().strftime(\\\"%H:%M:%S\\\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
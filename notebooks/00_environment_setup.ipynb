{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Medical Segmentation - Environment Setup\n",
    "\n",
    "This notebook sets up the complete environment for 3D medical image segmentation research.\n",
    "\n",
    "**Run this notebook first** before any training experiments.\n",
    "\n",
    "**Features:**\n",
    "- One-command automated setup\n",
    "- Environment verification\n",
    "- Dataset validation\n",
    "- GPU/CUDA testing\n",
    "- Project structure verification\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (recommended)\n",
    "- OR local Linux/macOS environment\n",
    "- Datasets uploaded to Google Drive (for Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Automated Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-command setup for Google Colab or local environment\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print('Starting 3D Medical Segmentation environment setup...')\n",
    "print('This will take 2-3 minutes with progress indicators.')\n",
    "print()\n",
    "\n",
    "# Try automated setup script first\n",
    "setup_url = 'https://raw.githubusercontent.com/Thabhelo/3d_medical_segemntation/main/setup.sh'\n",
    "\n",
    "# Detect environment\n",
    "is_colab = 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "setup_mode = '--colab' if is_colab else '--local'\n",
    "\n",
    "print(f'Detected environment: {\"Google Colab\" if is_colab else \"Local/Linux\"}')\n",
    "print(f'Running automated setup script...')\n",
    "print()\n",
    "\n",
    "# Run setup script\n",
    "result = subprocess.run(f'curl -sSL {setup_url} | bash -s -- {setup_mode}', shell=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print()\n",
    "    print('Automated setup completed successfully!')\n",
    "    print('Proceeding to verification...')\n",
    "else:\n",
    "    print()\n",
    "    print('Automated setup failed. Running manual setup...')\n",
    "    \n",
    "    # Manual fallback setup\n",
    "    if is_colab:\n",
    "        from google.colab import drive\n",
    "        print('Mounting Google Drive...')\n",
    "        drive.mount('/content/drive')\n",
    "        os.chdir('/content/drive/MyDrive')\n",
    "        \n",
    "        # Clone repository\n",
    "        repo_url = 'https://github.com/Thabhelo/3d_medical_segemntation.git'\n",
    "        if not os.path.exists('3d_medical_segmentation'):\n",
    "            print('Cloning repository...')\n",
    "            subprocess.run(['git', 'clone', repo_url, '3d_medical_segmentation'], check=True)\n",
    "        else:\n",
    "            print('Repository exists, updating...')\n",
    "            os.chdir('3d_medical_segmentation')\n",
    "            subprocess.run(['git', 'pull'], check=True)\n",
    "            os.chdir('..')\n",
    "        \n",
    "        os.chdir('3d_medical_segmentation')\n",
    "        \n",
    "        # Install dependencies\n",
    "        print('Installing dependencies...')\n",
    "        subprocess.run(['pip', 'install', '-q', '--upgrade', 'pip'], check=True)\n",
    "        subprocess.run(['pip', 'install', '-q', 'torch==2.3.0', 'torchvision==0.18.0'], check=True)\n",
    "        subprocess.run(['pip', 'install', '-q', 'monai[all]==1.3.0'], check=True)\n",
    "        subprocess.run(['pip', 'install', '-q', '-r', 'requirements.txt'], check=True)\n",
    "    \n",
    "    print('Manual setup completed')\n",
    "\n",
    "print()\n",
    "print('Setup phase completed! Proceeding to verification...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Python environment and core libraries\n",
    "import sys\n",
    "print('Python Environment:')\n",
    "print('=' * 50)\n",
    "print(f'Python version: {sys.version.split()[0]}')\n",
    "print(f'Platform: {sys.platform}')\n",
    "\n",
    "# Test core imports\n",
    "try:\n",
    "    import torch\n",
    "    print(f'PyTorch version: {torch.__version__}')\n",
    "    print(f'PyTorch CUDA compiled: {torch.version.cuda}')\n",
    "    print('PyTorch: OK')\n",
    "except ImportError as e:\n",
    "    print(f'PyTorch: FAILED - {e}')\n",
    "\n",
    "try:\n",
    "    import monai\n",
    "    print(f'MONAI version: {monai.__version__}')\n",
    "    print('MONAI: OK')\n",
    "except ImportError as e:\n",
    "    print(f'MONAI: FAILED - {e}')\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import scipy\n",
    "    import sklearn\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('Scientific libraries: OK')\n",
    "except ImportError as e:\n",
    "    print(f'Scientific libraries: FAILED - {e}')\n",
    "\n",
    "print()\n",
    "print('Core library verification completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: GPU and CUDA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU and CUDA verification\n",
    "import torch\n",
    "\n",
    "print('GPU and CUDA Status:')\n",
    "print('=' * 50)\n",
    "\n",
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f'CUDA available: {cuda_available}')\n",
    "\n",
    "if cuda_available:\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f'GPU count: {device_count}')\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f'GPU {i}: {gpu_name}')\n",
    "        print(f'  Memory: {gpu_memory:.1f} GB')\n",
    "    \n",
    "    # Test GPU computation\n",
    "    print()\n",
    "    print('Testing GPU computation...')\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.randn(1000, 1000, device=device)\n",
    "    y = torch.mm(x, x)\n",
    "    print('GPU computation test: PASSED')\n",
    "    \n",
    "    # Memory info\n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**2)\n",
    "    cached = torch.cuda.memory_reserved(0) / (1024**2)\n",
    "    print(f'GPU memory allocated: {allocated:.1f} MB')\n",
    "    print(f'GPU memory cached: {cached:.1f} MB')\n",
    "    \n",
    "    # Clear test tensors\n",
    "    del x, y\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print()\n",
    "    print('Recommendation: Use GPU training for optimal performance')\n",
    "else:\n",
    "    print('No GPU detected - will use CPU training')\n",
    "    print('Note: CPU training will be significantly slower (50-100x)')\n",
    "    print('Recommendation: Use Google Colab with GPU runtime')\n",
    "\n",
    "print()\n",
    "print('GPU verification completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Project Structure Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify project structure and components\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print('Project Structure Verification:')\n",
    "print('=' * 50)\n",
    "\n",
    "# Check working directory\n",
    "current_dir = Path.cwd()\n",
    "print(f'Current directory: {current_dir}')\n",
    "\n",
    "# Check essential directories\n",
    "essential_dirs = ['src', 'scripts', 'configs', 'notebooks']\n",
    "for dirname in essential_dirs:\n",
    "    dir_path = current_dir / dirname\n",
    "    if dir_path.exists():\n",
    "        file_count = len(list(dir_path.rglob('*.py')))\n",
    "        print(f'{dirname}/: OK ({file_count} Python files)')\n",
    "    else:\n",
    "        print(f'{dirname}/: MISSING')\n",
    "\n",
    "# Check essential files\n",
    "essential_files = ['requirements.txt', 'setup.py', 'README.md']\n",
    "for filename in essential_files:\n",
    "    file_path = current_dir / filename\n",
    "    if file_path.exists():\n",
    "        size_kb = file_path.stat().st_size / 1024\n",
    "        print(f'{filename}: OK ({size_kb:.1f} KB)')\n",
    "    else:\n",
    "        print(f'{filename}: MISSING')\n",
    "\n",
    "# Test model creation\n",
    "print()\n",
    "print('Testing model creation...')\n",
    "try:\n",
    "    import sys\n",
    "    if 'src' not in sys.path:\n",
    "        sys.path.append('src')\n",
    "    \n",
    "    from src.models.factory import create_model\n",
    "    \n",
    "    # Test each architecture\n",
    "    architectures = ['unet', 'unetr', 'segresnet']\n",
    "    for arch in architectures:\n",
    "        try:\n",
    "            model = create_model(architecture=arch, in_channels=1, out_channels=2)\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            print(f'{arch.upper()} model: OK ({param_count:,} parameters)')\n",
    "            del model  # Free memory\n",
    "        except Exception as e:\n",
    "            print(f'{arch.upper()} model: FAILED - {e}')\n",
    "            \n",
    "except ImportError as e:\n",
    "    print(f'Model factory import: FAILED - {e}')\n",
    "    print('Check if project structure is correct')\n",
    "\n",
    "print()\n",
    "print('Project structure verification completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset availability and structure\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print('Dataset Verification:')\n",
    "print('=' * 50)\n",
    "\n",
    "# Determine dataset root based on environment\n",
    "is_colab = 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "if is_colab:\n",
    "    datasets_root = Path('/content/drive/MyDrive/datasets')\n",
    "else:\n",
    "    datasets_root = Path.home() / 'Downloads' / 'datasets'\n",
    "\n",
    "print(f'Dataset root: {datasets_root}')\n",
    "print(f'Root exists: {datasets_root.exists()}')\n",
    "print()\n",
    "\n",
    "# Check each dataset\n",
    "datasets = {\n",
    "    'BraTS': ['BraTS', 'brats', 'BRATS'],\n",
    "    'MSD': ['MSD', 'MSD_Liver', 'Task03_Liver'],\n",
    "    'TotalSegmentator': ['TotalSegmentator', 'TotalSeg']\n",
    "}\n",
    "\n",
    "dataset_status = {}\n",
    "\n",
    "for canonical_name, possible_names in datasets.items():\n",
    "    found_path = None\n",
    "    \n",
    "    # Try to find dataset with any of the possible names\n",
    "    for name in possible_names:\n",
    "        candidate_path = datasets_root / name\n",
    "        if candidate_path.exists():\n",
    "            found_path = candidate_path\n",
    "            break\n",
    "    \n",
    "    if found_path:\n",
    "        # Count contents\n",
    "        try:\n",
    "            if canonical_name == 'BraTS':\n",
    "                # Count case directories\n",
    "                case_dirs = [d for d in found_path.rglob('*') if d.is_dir() and 'BraTS' in d.name]\n",
    "                count = len(case_dirs)\n",
    "                unit = 'cases'\n",
    "            else:\n",
    "                # Count files\n",
    "                files = list(found_path.rglob('*.nii.gz'))\n",
    "                count = len(files)\n",
    "                unit = 'NIfTI files'\n",
    "            \n",
    "            size_mb = sum(f.stat().st_size for f in found_path.rglob('*') if f.is_file()) / (1024 * 1024)\n",
    "            \n",
    "            print(f'{canonical_name}: FOUND')\n",
    "            print(f'  Path: {found_path}')\n",
    "            print(f'  Content: {count} {unit}')\n",
    "            print(f'  Size: {size_mb:.1f} MB')\n",
    "            dataset_status[canonical_name] = 'found'\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'{canonical_name}: FOUND (but error reading contents: {e})')\n",
    "            dataset_status[canonical_name] = 'error'\n",
    "    else:\n",
    "        print(f'{canonical_name}: MISSING')\n",
    "        print(f'  Expected at: {datasets_root / possible_names[0]}')\n",
    "        dataset_status[canonical_name] = 'missing'\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Summary\n",
    "found_count = sum(1 for status in dataset_status.values() if status == 'found')\n",
    "total_count = len(dataset_status)\n",
    "\n",
    "print(f'Dataset Summary: {found_count}/{total_count} datasets available')\n",
    "\n",
    "if found_count == 0:\n",
    "    print()\n",
    "    print('No datasets found! Please:')\n",
    "    if is_colab:\n",
    "        print('1. Upload datasets to Google Drive')\n",
    "        print('2. Organize them in /MyDrive/datasets/')\n",
    "    else:\n",
    "        print('1. Download datasets from official sources')\n",
    "        print('2. Place them in ~/Downloads/datasets/')\n",
    "    print('3. See datasets/README.md for download instructions')\n",
    "elif found_count < total_count:\n",
    "    print()\n",
    "    print('Some datasets missing - you can still run experiments with available datasets')\n",
    "else:\n",
    "    print()\n",
    "    print('All datasets available! Ready for full experiments.')\n",
    "\n",
    "print()\n",
    "print('Dataset verification completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Script Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training script functionality\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print('Training Script Verification:')\n",
    "print('=' * 50)\n",
    "\n",
    "# Check if training script exists\n",
    "train_script = Path('scripts/train_model.py')\n",
    "if not train_script.exists():\n",
    "    print('Training script not found!')\n",
    "    print('Expected location: scripts/train_model.py')\n",
    "else:\n",
    "    print(f'Training script found: {train_script}')\n",
    "    \n",
    "    # Test help command\n",
    "    try:\n",
    "        result = subprocess.run(['python', str(train_script), '--help'], \n",
    "                              capture_output=True, text=True, timeout=30)\n",
    "        if result.returncode == 0:\n",
    "            print('Training script help: OK')\n",
    "            \n",
    "            # Show available options\n",
    "            help_lines = result.stdout.split('\\n')\n",
    "            for line in help_lines:\n",
    "                if 'dataset' in line.lower() or 'architecture' in line.lower():\n",
    "                    print(f'  {line.strip()}')\n",
    "        else:\n",
    "            print('Training script help: FAILED')\n",
    "            print(f'Error: {result.stderr}')\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print('Training script help: TIMEOUT')\n",
    "    except Exception as e:\n",
    "        print(f'Training script help: ERROR - {e}')\n",
    "\n",
    "# Check configuration files\n",
    "print()\n",
    "print('Configuration files:')\n",
    "config_dir = Path('configs')\n",
    "if config_dir.exists():\n",
    "    config_files = list(config_dir.glob('*.yaml'))\n",
    "    for config_file in sorted(config_files):\n",
    "        print(f'  {config_file.name}')\n",
    "    print(f'Total: {len(config_files)} configuration files')\n",
    "else:\n",
    "    print('  No configs directory found')\n",
    "\n",
    "print()\n",
    "print('Training script verification completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Complete!\n",
    "\n",
    "### Summary Status\n",
    "Run the cell below to get a final summary of your setup status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final setup summary\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print('3D Medical Segmentation - Setup Summary')\n",
    "print('=' * 60)\n",
    "print()\n",
    "\n",
    "# Environment\n",
    "is_colab = 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "print(f'Environment: {\"Google Colab\" if is_colab else \"Local\"}')\n",
    "print(f'Python: {sys.version.split()[0]}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "\n",
    "# GPU Status\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f'GPU: {gpu_name} (CUDA available)')\n",
    "    print('Training mode: GPU (recommended)')\n",
    "else:\n",
    "    print('GPU: Not available')\n",
    "    print('Training mode: CPU (slower)')\n",
    "\n",
    "# Dataset status\n",
    "if is_colab:\n",
    "    datasets_root = Path('/content/drive/MyDrive/datasets')\n",
    "else:\n",
    "    datasets_root = Path.home() / 'Downloads' / 'datasets'\n",
    "\n",
    "dataset_count = 0\n",
    "if datasets_root.exists():\n",
    "    for name in ['BraTS', 'MSD', 'TotalSegmentator']:\n",
    "        if (datasets_root / name).exists():\n",
    "            dataset_count += 1\n",
    "\n",
    "print(f'Datasets: {dataset_count}/3 available')\n",
    "print(f'Dataset path: {datasets_root}')\n",
    "\n",
    "# Project status\n",
    "project_files = ['src', 'scripts', 'configs', 'requirements.txt']\n",
    "project_ok = all(Path(f).exists() for f in project_files)\n",
    "print(f'Project structure: {\"OK\" if project_ok else \"Issues detected\"}')\n",
    "\n",
    "print()\n",
    "print('Next Steps:')\n",
    "print('=' * 20)\n",
    "\n",
    "if dataset_count == 0:\n",
    "    print('1. Upload/download datasets (see dataset verification above)')\n",
    "    print('2. Run training notebooks once datasets are ready')\n",
    "elif dataset_count < 3:\n",
    "    print('1. Complete dataset setup (optional)')\n",
    "    print('2. Run training notebooks with available datasets')\n",
    "else:\n",
    "    print('1. Run training notebooks:')\n",
    "    print('   - 01_train_segresnet_experiments.ipynb')\n",
    "    print('   - 02_train_totalsegmentator_experiments.ipynb')\n",
    "    print('2. Analyze results when training completes')\n",
    "\n",
    "print()\n",
    "if torch.cuda.is_available() and dataset_count > 0:\n",
    "    print('Environment ready for training!')\n",
    "    print('Estimated time per experiment: 15-30 minutes')\n",
    "elif dataset_count > 0:\n",
    "    print('Environment ready (CPU training)')\n",
    "    print('Note: CPU training will be much slower')\n",
    "else:\n",
    "    print('Complete dataset setup to begin training')\n",
    "\n",
    "print()\n",
    "print('Setup completed successfully!')"
   ]
  }
 ],\n",
 \"metadata\": {\n",
  \"kernelspec\": {\n",
   \"display_name\": \"Python 3\",\n",
   \"language\": \"python\",\n",
   \"name\": \"python3\"\n",
  },\n,
  \"language_info\": {\n,
   \"codemirror_mode\": {\n,
    \"name\": \"ipython\",\n,
    \"version\": 3\n,
   },\n,
   \"file_extension\": \".py\",\n,
   \"mimetype\": \"text/x-python\",\n,
   \"name\": \"python\",\n,
   \"nbconvert_exporter\": \"python\",\n,
   \"pygments_lexer\": \"ipython3\",\n,
   \"version\": \"3.8.10\"\n,
  }\n,
 },\n,
 \"nbformat\": 4,\n,
 \"nbformat_minor\": 4\n,
}